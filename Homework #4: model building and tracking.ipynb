{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ac777b-e860-41a2-87f7-1c14f77960ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install MLflow (only needed if restarting the kernel or running for the first time)\n",
    "%pip install mlflow==1.14.0\n",
    "import mlflow\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2d408c-4b48-4d09-a238-4ec1031edba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up S3 client and bucket\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"columbia-gr5069-main\"\n",
    "\n",
    "# List of files you want to load from the bucket\n",
    "keys = {\n",
    "    \"drivers\": \"raw/drivers.csv\",\n",
    "    \"races\": \"raw/races.csv\",\n",
    "    \"results\": \"raw/results.csv\",\n",
    "    \"constructors\": \"raw/constructors.csv\",\n",
    "}\n",
    "\n",
    "# Dictionary to store the loaded DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through and load each CSV into a DataFrame\n",
    "for name, key in keys.items():\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    dataframes[name] = df\n",
    "    print(f\"Loaded {name} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "\n",
    "# Example usage:\n",
    "drivers_df = dataframes['drivers']\n",
    "races_df = dataframes['races']\n",
    "results_df = dataframes['results']\n",
    "constructors_df = dataframes['constructors']\n",
    "\n",
    "# Preview a DataFrame\n",
    "display(drivers_df)\n",
    "display(races_df)\n",
    "display(results_df)\n",
    "display(constructors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80526c99-59bb-4321-b8bb-bda5abaa0f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge four dataframes into one\n",
    "merged_df = pd.merge(results_df, races_df, on='raceId', how='left', suffixes=('', '_race'))\n",
    "merged_df = pd.merge(merged_df, drivers_df, on='driverId', how='left', suffixes=('', '_driver'))\n",
    "merged_df = pd.merge(merged_df, constructors_df, on='constructorId', how='left', suffixes=('', '_constructor'))\n",
    "merged_df['top_10'] = merged_df['positionOrder'] <= 10\n",
    "merged_df = merged_df[merged_df['positionOrder'].notnull()]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985d6a7b-cf2d-4396-bed5-63c8717f15c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df['dob'] = pd.to_datetime(merged_df['dob'], errors='coerce')\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'], errors='coerce')\n",
    "\n",
    "# Calculate driver age at time of race\n",
    "merged_df['driver_age'] = (merged_df['date'] - merged_df['dob']).dt.days // 365\n",
    "\n",
    "# Select modeling features\n",
    "features = [\n",
    "    'grid',\n",
    "    'constructorRef',\n",
    "    'nationality',\n",
    "    'driver_age',\n",
    "    'year',\n",
    "    'round'\n",
    "]\n",
    "\n",
    "# Drop rows with missing values in selected features\n",
    "model_df = merged_df[features + ['top_10']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df350cb-51d3-4a46-a418-0cb88aae2056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical = ['constructorRef', 'nationality']\n",
    "model_df = pd.get_dummies(model_df, columns=categorical, drop_first=True)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26fea8e5-48c7-4f4b-92c8-3f35179f0b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an experiment setup\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_df.drop('top_10', axis=1)\n",
    "y = model_df['top_10']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define tunable hyperparameters\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 5,\n",
    "    'criterion': 'gini'\n",
    "}\n",
    "\n",
    "clf = DecisionTreeClassifier(**params, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d57ee06-6916-418b-9d50-2f63b61910f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Convert target to int\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create 10 unique param combinations\n",
    "param_combinations = list(itertools.product(\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_samples_split'],\n",
    "    param_grid['min_samples_leaf'],\n",
    "    param_grid['criterion']\n",
    "))\n",
    "random.seed(42)\n",
    "sampled_combinations = random.sample(param_combinations, 10)\n",
    "\n",
    "# Ordinal names for model runs\n",
    "ordinals = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\",\n",
    "            \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\"]\n",
    "\n",
    "# Loop through 10 experiments\n",
    "for i, (max_depth, min_split, min_leaf, criterion) in enumerate(sampled_combinations):\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_split,\n",
    "        'min_samples_leaf': min_leaf,\n",
    "        'criterion': criterion\n",
    "    }\n",
    "    ordinal = ordinals[i]\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{ordinal.capitalize()} Run\"):\n",
    "        # Train model\n",
    "        clf = DecisionTreeClassifier(**params, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Log params & metrics\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"precision\", report['1']['precision'])\n",
    "        mlflow.log_metric(\"recall\", report['1']['recall'])\n",
    "\n",
    "        # Log model with ordinal name\n",
    "        mlflow.sklearn.log_model(clf, f\"{ordinal}_run_model\")\n",
    "\n",
    "        # Artifact 1: Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.title(f\"{ordinal.capitalize()} Run - Confusion Matrix\")\n",
    "        cm_path = f\"{ordinal}_confusion_matrix.png\"\n",
    "        plt.savefig(cm_path)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "\n",
    "        # Artifact 2: Predictions CSV\n",
    "        preds_df = X_test.copy()\n",
    "        preds_df[\"actual\"] = y_test\n",
    "        preds_df[\"predicted\"] = y_pred\n",
    "        csv_path = f\"{ordinal}_predictions.csv\"\n",
    "        preds_df.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(csv_path)\n",
    "\n",
    "        # Cleanup\n",
    "        os.remove(cm_path)\n",
    "        os.remove(csv_path)\n",
    "\n",
    "        print(f\"{ordinal.capitalize()} run completed — accuracy: {acc:.4f}, f1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833a6ef2-5503-4ee4-8196-dc4b8d83dbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Select your best model run and explain why\n",
    "\n",
    "Among the 10 decision tree models I trained, the best-performing run was the Fourth Run, which achieved an accuracy of 0.689, an F1 score of 0.617, precision of 0.637, and recall of 0.597.\n",
    "\n",
    "I selected this run as the best model because it had the highest F1 score, which reflects a strong balance between precision and recall — an important consideration when predicting top 10 finishes where false positives and false negatives both matter. It also tied for the best accuracy, making it a strong overall performer. And The best model used the following hyperparameters:\n",
    "\n",
    "- criterion: \"gini\"\n",
    "- max_depth: 5\n",
    "- min_samples_split: 2\n",
    "- min_samples_leaf: 1\n",
    "\n",
    "While my best model achieved moderate/weak performance (F1 score ~0.62), :(, this was expected given the limited features available. With additional data — such as qualifying times, lap performance, or historical driver stats — I believe the model could be significantly improved. Also, in reality, I would try several other models such as Random Forest or XGBoost to compare the result. Nonetheless, this process demonstrates a complete ML experimentation workflow, from feature selection to tracking and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc950e24-d8e6-45e5-82e3-bacd7c6a3d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Homework #4: model building and tracking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
